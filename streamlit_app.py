import streamlit as st
import cv2
import numpy as np
import time
import json
from threading import Thread, Lock
import queue
import mediapipe as mp
import dlib
import face_recognition
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import logging

# ë¡œê¹… ì„¤ì • ì¶”ê°€
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('AttendanceSystem')

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ ê°•ì˜ ì¶œì„ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AttendanceAnalyzer:
    def __init__(self, config=None):
        self.config = config  # config ì°¸ì¡° ì €ì¥
        try:
            # MediaPipe ì´ˆê¸°í™” (í•œë²ˆë§Œ ì´ˆê¸°í™” - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
            self.mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                static_image_mode=False,
                max_num_faces=1,
                refine_landmarks=True,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )

            # ë¶„ì„ ë³€ìˆ˜
            self.face_detected = False
            self.face_recognized = False
            self.blink_count = 0
            self.last_blink_time = time.time()
            self.head_pose_angles = {'yaw': 0, 'pitch': 0, 'roll': 0}
            self.attention_score = 0.0
            
            # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ Lock
            self.analysis_lock = Lock()

            # 3D ëª¨ë¸ í¬ì¸íŠ¸ (PnP ì†”ë²„ìš©)
            self.model_points = np.array([
                (0.0, 0.0, 0.0),             # ì½”ë
                (0.0, -330.0, -65.0),        # í„±
                (-225.0, 170.0, -135.0),     # ì™¼ìª½ ëˆˆ ì½”ë„ˆ
                (225.0, 170.0, -135.0),      # ì˜¤ë¥¸ìª½ ëˆˆ ì½”ë„ˆ
                (-150.0, -150.0, -125.0),    # ì™¼ìª½ ì… ì½”ë„ˆ
                (150.0, -150.0, -125.0)      # ì˜¤ë¥¸ìª½ ì… ì½”ë„ˆ
            ], dtype=np.float32)

            # ì¹´ë©”ë¼ ë§¤íŠ¸ë¦­ìŠ¤
            self.focal_length = 600.0
            self.camera_matrix = np.array([
                [self.focal_length, 0, 320],
                [0, self.focal_length, 240],
                [0, 0, 1]
            ], dtype=np.float32)
            self.dist_coeffs = np.zeros((4, 1))
            
            logger.info("AttendanceAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"AttendanceAnalyzer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise e

    def calculate_ear(self, landmarks, eye_points):
        try:
            eye = np.array([(landmarks[point].x, landmarks[point].y) for point in eye_points])
            A = np.linalg.norm(eye[1] - eye[5])
            B = np.linalg.norm(eye[2] - eye[4])
            C = np.linalg.norm(eye[0] - eye[3])
            ear = (A + B) / (2.0 * C) if C != 0 else 0.3
            logger.info(f"EAR ê°’: {ear:.3f}")  # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            return ear
        except Exception as e:
            logger.error(f"EAR ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.3

    def detect_blink(self, landmarks):
        # ìµœì‹  MediaPipe ê¸°ì¤€ ëˆˆ ì¢Œí‘œ ì¸ë±ìŠ¤ ì‚¬ìš©
        left_eye_points = [33, 160, 158, 133, 153, 144]
        right_eye_points = [362, 385, 387, 263, 373, 380]
        left_ear = self.calculate_ear(landmarks, left_eye_points)
        right_ear = self.calculate_ear(landmarks, right_eye_points)
        avg_ear = (left_ear + right_ear) / 2.0
        logger.info(f"í‰ê·  EAR: {avg_ear:.3f}")  # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        blink_threshold = 0.27  # ì„ê³„ê°’ ë‚®ì¶¤
        current_time = time.time()
        # ì¿¨íƒ€ì„ 0.3ì´ˆ ì ìš©
        if avg_ear < blink_threshold and (current_time - self.last_blink_time) > 0.3:
            self.last_blink_time = current_time
            return True
        return False

    def estimate_head_pose(self, landmarks, frame_shape):
        try:
            if len(landmarks) < 468:
                logger.warning("ëœë“œë§ˆí¬ ê°œìˆ˜ ë¶€ì¡±")
                return
            image_points = np.array([
                (landmarks[1].x * frame_shape[1], landmarks[1].y * frame_shape[0]),    # ì½”ë
                (landmarks[152].x * frame_shape[1], landmarks[152].y * frame_shape[0]), # í„±
                (landmarks[226].x * frame_shape[1], landmarks[226].y * frame_shape[0]), # ì™¼ìª½ ëˆˆ ì½”ë„ˆ
                (landmarks[446].x * frame_shape[1], landmarks[446].y * frame_shape[0]), # ì˜¤ë¥¸ìª½ ëˆˆ ì½”ë„ˆ
                (landmarks[57].x * frame_shape[1], landmarks[57].y * frame_shape[0]),   # ì™¼ìª½ ì… ì½”ë„ˆ
                (landmarks[287].x * frame_shape[1], landmarks[287].y * frame_shape[0])  # ì˜¤ë¥¸ìª½ ì… ì½”ë„ˆ
            ], dtype=np.float32)

            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.model_points, image_points, self.camera_matrix, self.dist_coeffs
            )

            if success:
                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
                sy = np.sqrt(rotation_matrix[0,0] * rotation_matrix[0,0] +  rotation_matrix[1,0] * rotation_matrix[1,0])

                singular = sy < 1e-6

                if not singular:
                    yaw = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
                    pitch = np.arctan2(-rotation_matrix[2,0], sy)
                    roll = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
                else:
                    yaw = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
                    pitch = np.arctan2(-rotation_matrix[2,0], sy)
                    roll = 0

                with self.analysis_lock:
                    self.head_pose_angles = {
                        'yaw': np.degrees(yaw),
                        'pitch': np.degrees(pitch),
                        'roll': np.degrees(roll)
                    }
                logger.info(f"Head Pose: yaw={self.head_pose_angles['yaw']:.1f}, pitch={self.head_pose_angles['pitch']:.1f}, roll={self.head_pose_angles['roll']:.1f}")
            else:
                logger.warning("solvePnP ì‹¤íŒ¨")
        except Exception as e:
            logger.error(f"ë¨¸ë¦¬ ìì„¸ ì¶”ì • ì¤‘ ì˜¤ë¥˜: {e}")

    def analyze_frame(self, frame):
        """í”„ë ˆì„ ë¶„ì„"""
        try:
            # í”„ë ˆì„ í¬ê¸° í™•ì¸
            if frame is None or frame.size == 0:
                logger.warning("ë¹ˆ í”„ë ˆì„ ìˆ˜ì‹ ")
                return self._get_default_result()

            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.face_mesh.process(rgb_frame)

            face_detected = False
            is_alive = False
            attention_focused = False

            if results.multi_face_landmarks:
                face_detected = True
                logger.debug("ì–¼êµ´ ê²€ì¶œë¨")

                for face_landmarks in results.multi_face_landmarks:
                    # ëˆˆ ê¹œë°•ì„ ê°ì§€
                    current_time = time.time()
                    if self.detect_blink(face_landmarks.landmark):
                        with self.analysis_lock:
                            self.blink_count += 1
                            self.last_blink_time = current_time
                        logger.debug(f"ëˆˆ ê¹œë°•ì„ ê°ì§€ (ì´ {self.blink_count}íšŒ)")
                    # ìµœê·¼ 2ì´ˆ ì´ë‚´ ê¹œë°•ì„ì´ ìˆìœ¼ë©´ is_alive True
                    with self.analysis_lock:
                        is_alive = (current_time - self.last_blink_time) < 2.0

                    # ë¨¸ë¦¬ ìì„¸ ì¶”ì •
                    self.estimate_head_pose(face_landmarks.landmark, frame.shape)

                    # ì£¼ì˜ì§‘ì¤‘ë„ íŒë‹¨ (roll ê°ë„ ë³´ì •, pitch í—ˆìš©ë²”ìœ„ ì™„í™”)
                    with self.analysis_lock:
                        yaw_ok = abs(self.head_pose_angles['yaw']) < 30
                        pitch_ok = abs(self.head_pose_angles['pitch']) < 40
                        roll_val = self.head_pose_angles['roll']
                        roll_ok = abs(roll_val) < 25 or abs(abs(roll_val) - 180) < 25
                        attention_focused = yaw_ok and pitch_ok and roll_ok
                    if attention_focused:
                        logger.debug("ì£¼ì˜ì§‘ì¤‘ ìƒíƒœ ì–‘í˜¸")
                    else:
                        logger.debug(f"ì£¼ì˜ì§‘ì¤‘ ì €í•˜: yaw={self.head_pose_angles['yaw']:.1f}, pitch={self.head_pose_angles['pitch']:.1f}, roll={self.head_pose_angles['roll']:.1f}")

            # ê°€ì¤‘ì¹˜/ì„ê³„ê°’ ì‹¤ì‹œê°„ ì ìš©
            cfg = self.config if self.config else {
                'face_detected_weight': 0.2,
                'face_recognized_weight': 0.3,
                'liveness_weight': 0.15,
                'attention_weight': 0.25,
                'head_pose_weight': 0.1,
                'excellent_threshold': 0.8,
                'present_threshold': 0.7,
                'attention_needed_threshold': 0.5
            }
            score_components = {
                'face_detected': cfg['face_detected_weight'] if face_detected else 0.0,
                'face_recognized': cfg['face_recognized_weight'] if face_detected else 0.0,
                'liveness': cfg['liveness_weight'] if is_alive else 0.0,
                'attention': cfg['attention_weight'] if attention_focused else 0.0,
                'head_pose': cfg['head_pose_weight'] if attention_focused else 0.0
            }
            total_score = sum(score_components.values())

            # ì¶œì„ ìƒíƒœ ê²°ì •
            if total_score >= cfg['excellent_threshold']:
                status = "excellent"
            elif total_score >= cfg['present_threshold']:
                status = "present"
            elif total_score >= cfg['attention_needed_threshold']:
                status = "attention_needed"
            else:
                status = "absent"
                
            logger.debug(f"ë¶„ì„ ì™„ë£Œ: ìƒíƒœ={status}, ì ìˆ˜={total_score:.2f}")

            with self.analysis_lock:
                return {
                    'face_detected': face_detected,
                    'face_recognized': face_detected,  # ê°„ë‹¨íˆ ì–¼êµ´ ê²€ì¶œë¡œ ëŒ€ì²´
                    'is_alive': is_alive,
                    'attention_focused': attention_focused,
                    'head_pose': self.head_pose_angles.copy(),
                    'blink_count': self.blink_count,
                    'attendance_score': total_score,
                    'attendance_status': status,
                    'timestamp': time.time()
                }
        except Exception as e:
            logger.error(f"í”„ë ˆì„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_default_result()

    def _get_default_result(self):
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜"""
        with self.analysis_lock:
            return {
                'face_detected': False,
                'face_recognized': False,
                'is_alive': False,
                'attention_focused': False,
                'head_pose': self.head_pose_angles.copy(),
                'blink_count': self.blink_count,
                'attendance_score': 0.0,
                'attendance_status': "error",
                'timestamp': time.time()
            }

# WebRTC ë¹„ë””ì˜¤ í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤
class VideoProcessor:
    def __init__(self, config=None):
        self.frame_count = 0
        self.analysis_interval = 5  # 5í”„ë ˆì„ë§ˆë‹¤ ë¶„ì„ (ì„±ëŠ¥ê³¼ ë°˜ì‘ì„± ê· í˜•)
        self.display_interval = 15  # 15í”„ë ˆì„ë§ˆë‹¤ í™”ë©´ í‘œì‹œ ì—…ë°ì´íŠ¸
        self.display_result = None  # í™”ë©´ í‘œì‹œìš© ê²°ê³¼ ì €ì¥
        self.config = config  # config ì°¸ì¡° ì €ì¥
        
        # ë¶„ì„ê¸° ì´ˆê¸°í™” (í•œë²ˆë§Œ)
        try:
            self.analyzer = AttendanceAnalyzer(config=self.config)
            logger.info("VideoProcessor ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"VideoProcessor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise e

        # ë¶„ì„ ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
        self.last_analysis = None
        self.analysis_lock = Lock()

    def draw_analysis_result(self, frame):
        """ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
        try:
            with self.analysis_lock:
                if not self.last_analysis:
                    return

                # í™”ë©´ í‘œì‹œ ì£¼ê¸°ì— ë”°ë¼ ê²°ê³¼ ì—…ë°ì´íŠ¸
                if self.frame_count % self.display_interval == 0:
                    self.display_result = self.last_analysis.copy()

                if not self.display_result:
                    return

                # ë°˜íˆ¬ëª…í•œ ë°°ê²½ ì¶”ê°€
                overlay = frame.copy()
                bg_height = 120  # ë°°ê²½ ë†’ì´
                cv2.rectangle(overlay, (5, 5), (400, bg_height), (0, 0, 0), -1)
                cv2.addWeighted(overlay, 0.3, frame, 0.7, 0, frame)

                # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì •
                status_color = {
                    "excellent": (0, 255, 0),    # ë…¹ìƒ‰
                    "present": (0, 255, 255),    # ë…¸ë€ìƒ‰
                    "attention_needed": (0, 165, 255),  # ì£¼í™©ìƒ‰
                    "absent": (0, 0, 255),       # ë¹¨ê°„ìƒ‰
                    "error": (128, 128, 128)     # íšŒìƒ‰
                }

                # ìƒíƒœ í‘œì‹œ
                color = status_color.get(self.display_result['attendance_status'], (128, 128, 128))
                
                # í…ìŠ¤íŠ¸ì— ê²€ì€ìƒ‰ ì™¸ê³½ì„  ì¶”ê°€
                def putTextWithOutline(img, text, pos, font, scale, color, thickness=2):
                    # ì™¸ê³½ì„  (ê²€ì€ìƒ‰)
                    cv2.putText(img, text, pos, font, scale, (0, 0, 0), thickness*3)
                    # ë‚´ë¶€ í…ìŠ¤íŠ¸
                    cv2.putText(img, text, pos, font, scale, color, thickness)

                # ì •ë³´ í‘œì‹œ (ì™¸ê³½ì„  ìˆëŠ” í…ìŠ¤íŠ¸)
                putTextWithOutline(frame, f"Status: {self.display_result['attendance_status'].upper()}", 
                        (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, color)
                putTextWithOutline(frame, f"Score: {self.display_result['attendance_score']:.2f}", 
                        (15, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, color)
                putTextWithOutline(frame, f"Blinks: {self.display_result['blink_count']}", 
                        (15, 105), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))

        except Exception as e:
            logger.error(f"ê²°ê³¼ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {e}")

    def recv(self, frame):
        """í”„ë ˆì„ ìˆ˜ì‹  ë° ì²˜ë¦¬"""
        try:
            # í”„ë ˆì„ ì¹´ìš´íŠ¸ ì¦ê°€
            self.frame_count += 1

            # ë¶„ì„ ê°„ê²©ë§ˆë‹¤ ì²˜ë¦¬
            if self.frame_count % self.analysis_interval == 0:
                img = frame.to_ndarray(format="bgr24")
                
                # í”„ë ˆì„ ë¶„ì„
                result = self.analyzer.analyze_frame(img)
                
                # ë¶„ì„ ê²°ê³¼ ì €ì¥ (ìŠ¤ë ˆë“œ ì•ˆì „)
                with self.analysis_lock:
                    self.last_analysis = result

                # ë¶„ì„ ê²°ê³¼ ì‹œê°í™”
                self.draw_analysis_result(img)
                
                # ì²˜ë¦¬ëœ í”„ë ˆì„ ë°˜í™˜
                return av.VideoFrame.from_ndarray(img, format="bgr24")
            
            # ë¶„ì„í•˜ì§€ ì•ŠëŠ” í”„ë ˆì„ë„ ì‹œê°í™”ëŠ” ìˆ˜í–‰
            img = frame.to_ndarray(format="bgr24")
            self.draw_analysis_result(img)
            return av.VideoFrame.from_ndarray(img, format="bgr24")

        except Exception as e:
            logger.error(f"í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return frame

    def get_last_analysis(self):
        """ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self.analysis_lock:
            if self.last_analysis:
                return self.last_analysis.copy()
            else:
                # ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    'face_detected': False,
                    'face_recognized': False,
                    'is_alive': False,
                    'attention_focused': False,
                    'head_pose': {'yaw': 0, 'pitch': 0, 'roll': 0},
                    'blink_count': 0,
                    'attendance_score': 0.0,
                    'attendance_status': "error",
                    'timestamp': time.time()
                }

def render_customization_panel(role, config):
    st.markdown("---")
    st.subheader("ë¶„ì„ ê¸°ì¤€ ì»¤ìŠ¤í„°ë§ˆì´ì§•")
    if role == "êµìˆ˜":
        st.info("ë¶„ì„ ê¸°ì¤€ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (êµìˆ˜ ì „ìš©)")
        config['face_detected_weight'] = st.slider("ì–¼êµ´ ê²€ì¶œ ê°€ì¤‘ì¹˜", 0.0, 1.0, config.get('face_detected_weight', 0.2), 0.05)
        config['face_recognized_weight'] = st.slider("ì–¼êµ´ ì¸ì‹ ê°€ì¤‘ì¹˜", 0.0, 1.0, config.get('face_recognized_weight', 0.3), 0.05)
        config['liveness_weight'] = st.slider("ìƒì²´ í™œë™ì„± ê°€ì¤‘ì¹˜", 0.0, 1.0, config.get('liveness_weight', 0.15), 0.05)
        config['attention_weight'] = st.slider("ì£¼ì˜ì§‘ì¤‘ ê°€ì¤‘ì¹˜", 0.0, 1.0, config.get('attention_weight', 0.25), 0.05)
        config['head_pose_weight'] = st.slider("ë¨¸ë¦¬ ìì„¸ ê°€ì¤‘ì¹˜", 0.0, 1.0, config.get('head_pose_weight', 0.1), 0.05)
        config['excellent_threshold'] = st.slider("'ìµœìš°ìˆ˜' ì„ê³„ê°’", 0.0, 1.0, config.get('excellent_threshold', 0.8), 0.01)
        config['present_threshold'] = st.slider("'ì¶œì„' ì„ê³„ê°’", 0.0, 1.0, config.get('present_threshold', 0.7), 0.01)
        config['attention_needed_threshold'] = st.slider("'ì£¼ì˜í•„ìš”' ì„ê³„ê°’", 0.0, 1.0, config.get('attention_needed_threshold', 0.5), 0.01)
    else:
        st.info("ì„¤ì •ê°’ì€ êµìˆ˜ë§Œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.write(f"ì–¼êµ´ ê²€ì¶œ ê°€ì¤‘ì¹˜: {config.get('face_detected_weight', 0.2)}")
        st.write(f"ì–¼êµ´ ì¸ì‹ ê°€ì¤‘ì¹˜: {config.get('face_recognized_weight', 0.3)}")
        st.write(f"ìƒì²´ í™œë™ì„± ê°€ì¤‘ì¹˜: {config.get('liveness_weight', 0.15)}")
        st.write(f"ì£¼ì˜ì§‘ì¤‘ ê°€ì¤‘ì¹˜: {config.get('attention_weight', 0.25)}")
        st.write(f"ë¨¸ë¦¬ ìì„¸ ê°€ì¤‘ì¹˜: {config.get('head_pose_weight', 0.1)}")
        st.write(f"'ìµœìš°ìˆ˜' ì„ê³„ê°’: {config.get('excellent_threshold', 0.8)}")
        st.write(f"'ì¶œì„' ì„ê³„ê°’: {config.get('present_threshold', 0.7)}")
        st.write(f"'ì£¼ì˜í•„ìš”' ì„ê³„ê°’: {config.get('attention_needed_threshold', 0.5)}")

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    st.title("ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ ê°•ì˜ ì¶œì„ ì‹œìŠ¤í…œ")
    st.markdown("""
    ì´ ì‹œìŠ¤í…œì€ ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìƒì˜ ì¶œì„ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.
    - ğŸ‘¤ **ì–¼êµ´ ê²€ì¶œ**: í•™ìƒì˜ ì–¼êµ´ì´ í™”ë©´ì— ìˆëŠ”ì§€ í™•ì¸
    - ğŸ‘ï¸ **ìƒì²´ í™œë™ì„±**: ëˆˆ ê¹œë°•ì„ì„ í†µí•œ ì‹¤ì œ ì‚¬ëŒ ì—¬ë¶€ í™•ì¸
    - ğŸ¯ **ì£¼ì˜ì§‘ì¤‘ë„**: ë¨¸ë¦¬ ìì„¸ë¥¼ í†µí•œ ìˆ˜ì—… ì§‘ì¤‘ë„ ì¸¡ì •
    """)

    # ì—­í•  ì„ íƒ
    role = st.sidebar.selectbox("ì—­í• ì„ ì„ íƒí•˜ì„¸ìš”", ["êµìˆ˜", "í•™ìƒ"])

    # ë¶„ì„ íŒŒë¼ë¯¸í„° config (ì„¸ì…˜ ìƒíƒœë¡œ ê´€ë¦¬)
    if 'analyze_config' not in st.session_state:
        st.session_state['analyze_config'] = {
            'face_detected_weight': 0.2,
            'face_recognized_weight': 0.3,
            'liveness_weight': 0.15,
            'attention_weight': 0.25,
            'head_pose_weight': 0.1,
            'excellent_threshold': 0.8,
            'present_threshold': 0.7,
            'attention_needed_threshold': 0.5
        }
    config = st.session_state['analyze_config']

    # ì»¤ìŠ¤í„°ë§ˆì´ì§• íŒ¨ë„
    render_customization_panel(role, config)

    # STUN ì„œë²„ ì„¤ì •
    RTC_CONFIGURATION = RTCConfiguration(
        {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )

    # ë ˆì´ì•„ì›ƒ
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ“¹ ì›¹ìº  í™”ë©´")
        webrtc_ctx = webrtc_streamer(
            key="attendance-analyzer",
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=RTC_CONFIGURATION,
            video_processor_factory=lambda: VideoProcessor(config=config),
            media_stream_constraints={"video": True, "audio": False},
            async_processing=False,  # ë™ê¸° ì²˜ë¦¬ë¡œ ë³€ê²½
        )

    with col2:
        st.subheader("ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼")

        status_placeholder = st.empty()
        metrics_placeholder = st.empty()

        # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë£¨í”„
        if webrtc_ctx.state.playing and webrtc_ctx.video_processor:
            while True:
                try:
                    result = webrtc_ctx.video_processor.get_last_analysis()
                    if result:
                        with status_placeholder.container():
                            status_color = {
                                "excellent": "ğŸŸ¢",
                                "present": "ğŸŸ¡", 
                                "attention_needed": "ğŸŸ ",
                                "absent": "ğŸ”´",
                                "error": "âš«"
                            }
                            st.metric(
                                "ì¶œì„ ìƒíƒœ", 
                                f"{status_color.get(result.get('attendance_status', 'error'), 'âšª')} {result.get('attendance_status', 'ERROR').upper()}",
                                f"ì ìˆ˜: {result.get('attendance_score', 0.0):.2f}"
                            )
                        with metrics_placeholder.container():
                            col_a, col_b = st.columns(2)
                            with col_a:
                                st.metric("ì–¼êµ´ ê²€ì¶œ", "âœ…" if result.get('face_detected') else "âŒ")
                                st.metric("ìƒì²´ í™œë™ì„±", "âœ…" if result.get('is_alive') else "âŒ")
                            with col_b:
                                st.metric("ì–¼êµ´ ì¸ì‹", "âœ…" if result.get('face_recognized') else "âŒ")
                                st.metric("ì£¼ì˜ì§‘ì¤‘", "âœ…" if result.get('attention_focused') else "âŒ")
                            st.subheader("ë¨¸ë¦¬ ìì„¸")
                            pose = result.get('head_pose', {'yaw':0, 'pitch':0, 'roll':0})
                            st.write(f"**Yaw:** {pose.get('yaw',0):.1f}Â°")
                            st.write(f"**Pitch:** {pose.get('pitch',0):.1f}Â°") 
                            st.write(f"**Roll:** {pose.get('roll',0):.1f}Â°")
                            st.metric("ëˆˆ ê¹œë°•ì„", result.get('blink_count', 0))
                    else:
                        with status_placeholder.container():
                            st.info("ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
                    time.sleep(0.2)
                except Exception as e:
                    logger.error(f"ê²°ê³¼ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}")
                    with status_placeholder.container():
                        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    break
                if not webrtc_ctx.state.playing:
                    break
        else:
            with status_placeholder.container():
                st.info("ì›¹ìº ì„ ì‹œì‘í•˜ë ¤ë©´ 'START' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")

if __name__ == "__main__":
    main()