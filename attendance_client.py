import asyncio
import socketio
import cv2
import dlib
import numpy as np
import mediapipe as mp
import face_recognition
import time
import json
from threading import Thread
import queue

# ì„œë²„ ì„¤ì •
SERVER_URL = "http://localhost:3000"

class RealTimeAttendanceClient:
    def __init__(self, student_id, student_name):
        # Socket.IO í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.sio = socketio.AsyncClient(
            ssl_verify=False,
            engineio_logger=False
        )

        # í•™ìƒ ì •ë³´
        self.student_id = student_id
        self.student_name = student_name
        self.room_id = f"class_{student_id}"

        # MediaPipe ì´ˆê¸°í™” (ë” ê°„ë‹¨í•œ ì„¤ì •)
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        # ì¹´ë©”ë¼ ì„¤ì •
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.cap.set(cv2.CAP_PROP_FPS, 30)

        # ë¶„ì„ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.blink_count = 0
        self.last_blink_time = time.time()
        self.head_pose_angles = {'yaw': 0, 'pitch': 0, 'roll': 0}

        # ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ í
        self.frame_queue = queue.Queue(maxsize=5)
        self.analysis_running = True

        # 3D ëª¨ë¸ í¬ì¸íŠ¸ (PnP ì†”ë²„ìš©)
        self.model_points = np.array([
            (0.0, 0.0, 0.0),             # ì½”ë
            (0.0, -330.0, -65.0),        # í„±
            (-225.0, 170.0, -135.0),     # ì™¼ìª½ ëˆˆ ì½”ë„ˆ
            (225.0, 170.0, -135.0),      # ì˜¤ë¥¸ìª½ ëˆˆ ì½”ë„ˆ
            (-150.0, -150.0, -125.0),    # ì™¼ìª½ ì… ì½”ë„ˆ
            (150.0, -150.0, -125.0)      # ì˜¤ë¥¸ìª½ ì… ì½”ë„ˆ
        ], dtype=np.float32)

        # ì¹´ë©”ë¼ ë§¤íŠ¸ë¦­ìŠ¤
        self.focal_length = 600.0
        self.camera_matrix = np.array([
            [self.focal_length, 0, 320],
            [0, self.focal_length, 240],
            [0, 0, 1]
        ], dtype=np.float32)
        self.dist_coeffs = np.zeros((4, 1))

        # ì¶œì„ ìƒíƒœ ë³€ìˆ˜
        self.attendance_status = "not_detected"
        self.last_update_time = time.time()

        # Socket.IO ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡
        self.setup_socket_events()

    def setup_socket_events(self):
        """Socket.IO ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        @self.sio.event
        async def connect():
            print(f"ğŸ”— ì„œë²„ì— ì—°ê²°ë¨ - í•™ìƒ: {self.student_name}")
            await self.join_classroom()

        @self.sio.event
        async def disconnect():
            print("âŒ ì„œë²„ ì—°ê²° í•´ì œ")

        @self.sio.on("attendanceUpdate")
        async def on_attendance_update(data):
            print(f"ğŸ“Š ì¶œì„ ìƒíƒœ ì—…ë°ì´íŠ¸: {data}")

        @self.sio.on("classroomJoined")
        async def on_classroom_joined(data):
            print(f"ğŸ“ ê°•ì˜ì‹¤ ì…ì¥ ì™„ë£Œ: {data}")

        @self.sio.on("liveAnalysisResult")
        async def on_live_analysis(data):
            # ë‹¤ë¥¸ í•™ìƒë“¤ì˜ ë¶„ì„ ê²°ê³¼ëŠ” ê°„ë‹¨íˆ ë¡œê·¸ë§Œ
            pass

    async def join_classroom(self):
        """ê°•ì˜ì‹¤ ì…ì¥"""
        join_data = {
            'student_id': self.student_id,
            'student_name': self.student_name,
            'room_id': self.room_id,
            'timestamp': time.time()
        }
        await self.sio.emit('joinClassroom', join_data)

    def calculate_ear(self, landmarks, eye_points):
        """Eye Aspect Ratio ê³„ì‚° (ëˆˆ ê¹œë°•ì„ ê°ì§€)"""
        if len(eye_points) < 6:
            return 0.3  # ê¸°ë³¸ê°’

        try:
            eye = np.array([(landmarks[point].x, landmarks[point].y) for point in eye_points[:6]])
            A = np.linalg.norm(eye[1] - eye[5])
            B = np.linalg.norm(eye[2] - eye[4])
            C = np.linalg.norm(eye[0] - eye[3])
            ear = (A + B) / (2.0 * C)
            return ear
        except:
            return 0.3

    def detect_blink(self, landmarks):
        """ëˆˆ ê¹œë°•ì„ ê°ì§€"""
        # MediaPipe ì–¼êµ´ ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (ê°„ì†Œí™”)
        left_eye_points = [33, 7, 163, 144, 145, 153]
        right_eye_points = [362, 382, 381, 380, 374, 373]

        left_ear = self.calculate_ear(landmarks, left_eye_points)
        right_ear = self.calculate_ear(landmarks, right_eye_points)
        avg_ear = (left_ear + right_ear) / 2.0

        return avg_ear < 0.25

    def estimate_head_pose(self, landmarks, frame_shape):
        """ë¨¸ë¦¬ ìì„¸ ì¶”ì • (ê°„ì†Œí™”)"""
        try:
            # 2D ì´ë¯¸ì§€ í¬ì¸íŠ¸ ì¶”ì¶œ
            image_points = np.array([
                (landmarks[1].x * frame_shape[1], landmarks[1].y * frame_shape[0]),    # ì½”ë
                (landmarks[152].x * frame_shape[1], landmarks[152].y * frame_shape[0]), # í„±
                (landmarks[226].x * frame_shape[1], landmarks[226].y * frame_shape[0]), # ì™¼ìª½ ëˆˆ ì½”ë„ˆ
                (landmarks[446].x * frame_shape[1], landmarks[446].y * frame_shape[0]), # ì˜¤ë¥¸ìª½ ëˆˆ ì½”ë„ˆ
                (landmarks[57].x * frame_shape[1], landmarks[57].y * frame_shape[0]),   # ì™¼ìª½ ì… ì½”ë„ˆ
                (landmarks[287].x * frame_shape[1], landmarks[287].y * frame_shape[0])  # ì˜¤ë¥¸ìª½ ì… ì½”ë„ˆ
            ], dtype=np.float32)

            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.model_points, image_points, self.camera_matrix, self.dist_coeffs
            )

            if success:
                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
                sy = np.sqrt(rotation_matrix[0,0] * rotation_matrix[0,0] +  rotation_matrix[1,0] * rotation_matrix[1,0])

                if sy > 1e-6:
                    yaw = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
                    pitch = np.arctan2(-rotation_matrix[2,0], sy)
                    roll = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
                else:
                    yaw = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
                    pitch = np.arctan2(-rotation_matrix[2,0], sy)
                    roll = 0

                self.head_pose_angles = {
                    'yaw': np.degrees(yaw),
                    'pitch': np.degrees(pitch),
                    'roll': np.degrees(roll)
                }
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ìœ ì§€
            pass

    def analyze_frame(self, frame):
        """í”„ë ˆì„ ì‹¤ì‹œê°„ ë¶„ì„"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_frame)

        face_detected = False
        face_recognized = False
        is_alive = False
        attention_focused = False

        if results.multi_face_landmarks:
            face_detected = True
            face_recognized = True  # ê°„ë‹¨íˆ ì–¼êµ´ ê²€ì¶œë¡œ ëŒ€ì²´

            for face_landmarks in results.multi_face_landmarks:
                # ëˆˆ ê¹œë°•ì„ ê°ì§€
                if self.detect_blink(face_landmarks.landmark):
                    current_time = time.time()
                    if current_time - self.last_blink_time > 0.3:
                        self.blink_count += 1
                        self.last_blink_time = current_time
                        is_alive = True

                # ë¨¸ë¦¬ ìì„¸ ì¶”ì •
                self.estimate_head_pose(face_landmarks.landmark, frame.shape)

                # ì£¼ì˜ì§‘ì¤‘ë„ íŒë‹¨ (ë¨¸ë¦¬ ìì„¸ ê¸°ì¤€)
                yaw_ok = abs(self.head_pose_angles['yaw']) < 30
                pitch_ok = abs(self.head_pose_angles['pitch']) < 20
                roll_ok = abs(self.head_pose_angles['roll']) < 25
                attention_focused = yaw_ok and pitch_ok and roll_ok

        # ì¢…í•© ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°
        score_components = {
            'face_detected': 0.2 if face_detected else 0.0,
            'face_recognized': 0.3 if face_recognized else 0.0,
            'liveness': 0.15 if is_alive else 0.0,
            'attention': 0.25 if attention_focused else 0.0,
            'head_pose': 0.1 if attention_focused else 0.0
        }

        total_score = sum(score_components.values())

        # ì¶œì„ ìƒíƒœ ê²°ì •
        if total_score >= 0.8:
            status = "excellent"
        elif total_score >= 0.7:
            status = "present"
        elif total_score >= 0.5:
            status = "attention_needed"
        else:
            status = "absent"

        return {
            'face_detected': face_detected,
            'face_recognized': face_recognized,
            'is_alive': is_alive,
            'attention_focused': attention_focused,
            'head_pose': self.head_pose_angles,
            'blink_count': self.blink_count,
            'attendance_score': total_score,
            'attendance_status': status,
            'timestamp': time.time()
        }

    async def send_realtime_update(self, analysis_result):
        """ì‹¤ì‹œê°„ ì¶œì„ ìƒíƒœ ì—…ë°ì´íŠ¸ ì „ì†¡"""
        update_data = {
            'student_id': self.student_id,
            'room_id': self.room_id,
            'analysis': analysis_result,
            'session_time': time.time() - self.last_update_time
        }

        try:
            await self.sio.emit('realtimeAttendanceUpdate', update_data)
        except Exception as e:
            print(f"âŒ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {e}")

    def frame_capture_thread(self):
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í”„ë ˆì„ ìº¡ì²˜"""
        while self.analysis_running:
            ret, frame = self.cap.read()
            if ret:
                if self.frame_queue.full():
                    try:
                        self.frame_queue.get_nowait()
                    except queue.Empty:
                        pass

                try:
                    self.frame_queue.put_nowait(frame)
                except queue.Full:
                    pass

            time.sleep(0.033)  # ì•½ 30 FPS

    async def realtime_analysis_loop(self):
        """ì‹¤ì‹œê°„ ë¶„ì„ ë©”ì¸ ë£¨í”„"""
        # í”„ë ˆì„ ìº¡ì²˜ ìŠ¤ë ˆë“œ ì‹œì‘
        capture_thread = Thread(target=self.frame_capture_thread)
        capture_thread.daemon = True
        capture_thread.start()

        print("ğŸ¥ ì‹¤ì‹œê°„ ì¶œì„ ë¶„ì„ ì‹œì‘...")

        while self.analysis_running:
            try:
                if not self.frame_queue.empty():
                    frame = self.frame_queue.get_nowait()

                    # í”„ë ˆì„ ë¶„ì„
                    analysis_result = self.analyze_frame(frame)

                    # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì „ì†¡ (ë§¤ í”„ë ˆì„ë§ˆë‹¤)
                    await self.send_realtime_update(analysis_result)

                    # í™”ë©´ì— ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                    self.display_analysis_result(frame, analysis_result)

                    # í”„ë ˆì„ ì¶œë ¥
                    cv2.imshow('Real-time Attendance Monitor', frame)

                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break

                await asyncio.sleep(0.01)  # 10ms

            except queue.Empty:
                await asyncio.sleep(0.01)
            except Exception as e:
                print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(0.1)

    def display_analysis_result(self, frame, result):
        """ë¶„ì„ ê²°ê³¼ë¥¼ í”„ë ˆì„ì— í‘œì‹œ"""
        status_colors = {
            "excellent": (0, 255, 0),
            "present": (0, 200, 255),
            "attention_needed": (0, 165, 255),
            "absent": (0, 0, 255)
        }

        status = result['attendance_status']
        color = status_colors.get(status, (255, 255, 255))

        # ì •ë³´ í‘œì‹œ
        cv2.putText(frame, f"Status: {status.upper()}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        cv2.putText(frame, f"Score: {result['attendance_score']:.2f}", (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(frame, f"Face: {'âœ“' if result['face_detected'] else 'âœ—'}", (10, 90), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0) if result['face_detected'] else (0, 0, 255), 2)
        cv2.putText(frame, f"Alive: {'âœ“' if result['is_alive'] else 'âœ—'}", (10, 120), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0) if result['is_alive'] else (0, 0, 255), 2)
        cv2.putText(frame, f"Attention: {'âœ“' if result['attention_focused'] else 'âœ—'}", (10, 150), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0) if result['attention_focused'] else (0, 0, 255), 2)

        # ë¨¸ë¦¬ ìì„¸ ì •ë³´
        pose = result['head_pose']
        cv2.putText(frame, f"Yaw: {pose['yaw']:.1f}Â°", (10, 220), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        cv2.putText(frame, f"Pitch: {pose['pitch']:.1f}Â°", (10, 240), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        cv2.putText(frame, f"Roll: {pose['roll']:.1f}Â°", (10, 260), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

    async def connect_and_start(self):
        """ì„œë²„ ì—°ê²° ë° ì‹¤ì‹œê°„ ë¶„ì„ ì‹œì‘"""
        try:
            print(f"ğŸ”„ ì„œë²„ ì—°ê²° ì‹œë„ ì¤‘: {SERVER_URL}")
            await self.sio.connect(
                SERVER_URL,
                transports=['websocket'],
                headers={'Content-Type': 'application/json'}
            )

            await self.realtime_analysis_loop()

        except Exception as e:
            print(f"âŒ ì—°ê²° ë˜ëŠ” ë¶„ì„ ì˜¤ë¥˜: {e}")
        finally:
            await self.cleanup()

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.analysis_running = False
        self.cap.release()
        cv2.destroyAllWindows()
        if self.sio.connected:
            await self.sio.disconnect()
        print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def main():
    # í•™ìƒ ì •ë³´ ì„¤ì •
    STUDENT_ID = input("í•™ìƒ IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 2024001): ").strip() or "2024001"
    STUDENT_NAME = input("í•™ìƒ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: ê¹€ì² ìˆ˜): ").strip() or "ê¹€ì² ìˆ˜"

    print(f"\nğŸ“ í•™ìƒ ì •ë³´:")
    print(f"   ID: {STUDENT_ID}")
    print(f"   ì´ë¦„: {STUDENT_NAME}")
    print(f"\nğŸ“‹ ì‚¬ìš© ë°©ë²•:")
    print("   - ì›¹ìº  ì°½ì—ì„œ 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print("   - ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    print("\nğŸš€ ì‹œìŠ¤í…œ ì‹œì‘ ì¤‘...\n")

    # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = RealTimeAttendanceClient(STUDENT_ID, STUDENT_NAME)

    # ì—°ê²° ë° ì‹¤ì‹œê°„ ë¶„ì„ ì‹œì‘
    await client.connect_and_start()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì˜¤ë¥˜: {e}")
