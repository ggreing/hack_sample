import streamlit as st
import cv2
import numpy as np
import time
import json
from threading import Thread, Lock
import queue
import mediapipe as mp
import dlib
import face_recognition
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import logging
import socketio
import asyncio
from concurrent.futures import ThreadPoolExecutor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('AttendanceSystem')

# ì„œë²„ ì„¤ì •
SERVER_URL = "http://13.238.227.125:3000"

# SocketIO í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤
class SocketIOClient:
    def __init__(self):
        self.sio = socketio.Client()
        self.connected = False
        self.setup_events()
        
    def setup_events(self):
        @self.sio.event
        def connect():
            logger.info("SocketIO ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤")
            self.connected = True
            
        @self.sio.event
        def disconnect():
            logger.info("SocketIO ì„œë²„ì—ì„œ ì—°ê²°ì´ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤")
            self.connected = False
            
        @self.sio.on('attendance_response')
        def on_attendance_response(data):
            logger.info(f"ì„œë²„ ì‘ë‹µ: {data}")
            
    def connect_to_server(self):
        try:
            if not self.connected:
                self.sio.connect(SERVER_URL, transports=['websocket'])
                return True
        except Exception as e:
            logger.error(f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
        return self.connected
    
    def send_attendance_data(self, student_data, analysis_result):
        """ì¶œì„ ë°ì´í„°ë¥¼ ì„œë²„ë¡œ ì „ì†¡"""
        if self.connected:
            try:
                payload = {
                    'student_id': student_data.get('id'),
                    'student_name': student_data.get('name'),
                    'timestamp': time.time(),
                    'analysis_result': analysis_result,
                    'endpoint': '/api/attendance'
                }
                self.sio.emit('attendance_update', payload)
                logger.info(f"ì¶œì„ ë°ì´í„° ì „ì†¡ ì™„ë£Œ: {student_data.get('name')}")
                return True
            except Exception as e:
                logger.error(f"ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨: {e}")
                return False
        return False
    
    def disconnect_from_server(self):
        if self.connected:
            self.sio.disconnect()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ ê°•ì˜ ì¶œì„ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AttendanceAnalyzer:
    def __init__(self, criteria_weights=None):
        try:
            # MediaPipe ì´ˆê¸°í™”
            self.mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                static_image_mode=False,
                max_num_faces=1,
                refine_landmarks=True,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            
            # íŒë‹¨ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ì„¤ì • (ê¸°ë³¸ê°’)
            self.criteria_weights = criteria_weights or {
                'face_detected': 0.2,
                'face_recognized': 0.3,
                'liveness': 0.15,
                'attention': 0.25,
                'head_pose': 0.1
            }
            
            # ë¶„ì„ ë³€ìˆ˜
            self.face_detected = False
            self.face_recognized = False
            self.blink_count = 0
            self.last_blink_time = time.time()
            self.head_pose_angles = {'yaw': 0, 'pitch': 0, 'roll': 0}
            self.attention_score = 0.0
            
            # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ Lock
            self.analysis_lock = Lock()
            
            # 3D ëª¨ë¸ í¬ì¸íŠ¸ (PnP ì†”ë²„ìš©)
            self.model_points = np.array([
                (0.0, 0.0, 0.0),
                (0.0, -330.0, -65.0),
                (-225.0, 170.0, -135.0),
                (225.0, 170.0, -135.0),
                (-150.0, -150.0, -125.0),
                (150.0, -150.0, -125.0)
            ], dtype=np.float32)
            
            # ì¹´ë©”ë¼ ë§¤íŠ¸ë¦­ìŠ¤
            self.focal_length = 600.0
            self.camera_matrix = np.array([
                [self.focal_length, 0, 320],
                [0, self.focal_length, 240],
                [0, 0, 1]
            ], dtype=np.float32)
            self.dist_coeffs = np.zeros((4, 1))
            
            logger.info("AttendanceAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"AttendanceAnalyzer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise e
    
    def update_criteria_weights(self, new_weights):
        """íŒë‹¨ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        with self.analysis_lock:
            self.criteria_weights = new_weights
            logger.info(f"íŒë‹¨ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸: {new_weights}")
    
    def calculate_ear(self, landmarks, eye_points):
        try:
            eye = np.array([(landmarks[point].x, landmarks[point].y) for point in eye_points])
            A = np.linalg.norm(eye[1] - eye[5])
            B = np.linalg.norm(eye[2] - eye[4])
            C = np.linalg.norm(eye[0] - eye[3])
            ear = (A + B) / (2.0 * C) if C != 0 else 0.3
            return ear
        except Exception as e:
            logger.error(f"EAR ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.3
    
    def detect_blink(self, landmarks):
        left_eye_points = [33, 160, 158, 133, 153, 144]
        right_eye_points = [362, 385, 387, 263, 373, 380]
        
        left_ear = self.calculate_ear(landmarks, left_eye_points)
        right_ear = self.calculate_ear(landmarks, right_eye_points)
        avg_ear = (left_ear + right_ear) / 2.0
        
        blink_threshold = 0.27
        current_time = time.time()
        
        if avg_ear < blink_threshold and (current_time - self.last_blink_time) > 0.3:
            self.last_blink_time = current_time
            return True
        return False
    
    def estimate_head_pose(self, landmarks, frame_shape):
        try:
            if len(landmarks) < 468:
                logger.warning("ëœë“œë§ˆí¬ ê°œìˆ˜ ë¶€ì¡±")
                return
                
            image_points = np.array([
                (landmarks[1].x * frame_shape[1], landmarks[1].y * frame_shape[0]),
                (landmarks[152].x * frame_shape[1], landmarks[152].y * frame_shape[0]),
                (landmarks[226].x * frame_shape[1], landmarks[226].y * frame_shape[0]),
                (landmarks[446].x * frame_shape[1], landmarks[446].y * frame_shape[0]),
                (landmarks[57].x * frame_shape[1], landmarks[57].y * frame_shape[0]),
                (landmarks[287].x * frame_shape[1], landmarks[287].y * frame_shape[0])
            ], dtype=np.float32)
            
            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.model_points, image_points, self.camera_matrix, self.dist_coeffs
            )
            
            if success:
                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
                sy = np.sqrt(rotation_matrix[0,0] * rotation_matrix[0,0] + rotation_matrix[1,0] * rotation_matrix[1,0])
                
                singular = sy < 1e-6
                if not singular:
                    yaw = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
                    pitch = np.arctan2(-rotation_matrix[2,0], sy)
                    roll = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
                else:
                    yaw = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
                    pitch = np.arctan2(-rotation_matrix[2,0], sy)
                    roll = 0
                
                with self.analysis_lock:
                    self.head_pose_angles = {
                        'yaw': np.degrees(yaw),
                        'pitch': np.degrees(pitch),
                        'roll': np.degrees(roll)
                    }
        except Exception as e:
            logger.error(f"ë¨¸ë¦¬ ìì„¸ ì¶”ì • ì¤‘ ì˜¤ë¥˜: {e}")
    
    def analyze_frame(self, frame):
        """í”„ë ˆì„ ë¶„ì„"""
        try:
            if frame is None or frame.size == 0:
                logger.warning("ë¹ˆ í”„ë ˆì„ ìˆ˜ì‹ ")
                return self._get_default_result()
                
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.face_mesh.process(rgb_frame)
            
            face_detected = False
            is_alive = False
            attention_focused = False
            
            if results.multi_face_landmarks:
                face_detected = True
                
                for face_landmarks in results.multi_face_landmarks:
                    current_time = time.time()
                    if self.detect_blink(face_landmarks.landmark):
                        with self.analysis_lock:
                            self.blink_count += 1
                            self.last_blink_time = current_time
                    
                    with self.analysis_lock:
                        is_alive = (current_time - self.last_blink_time) < 2.0
                    
                    self.estimate_head_pose(face_landmarks.landmark, frame.shape)
                    
                    with self.analysis_lock:
                        yaw_ok = abs(self.head_pose_angles['yaw']) < 30
                        pitch_ok = abs(self.head_pose_angles['pitch']) < 40
                        roll_val = self.head_pose_angles['roll']
                        roll_ok = abs(roll_val) < 25 or abs(abs(roll_val) - 180) < 25
                        attention_focused = yaw_ok and pitch_ok and roll_ok
            
            # ë™ì  ì ìˆ˜ ê³„ì‚° (ì„¤ì •ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
            with self.analysis_lock:
                score_components = {
                    'face_detected': self.criteria_weights['face_detected'] if face_detected else 0.0,
                    'face_recognized': self.criteria_weights['face_recognized'] if face_detected else 0.0,
                    'liveness': self.criteria_weights['liveness'] if is_alive else 0.0,
                    'attention': self.criteria_weights['attention'] if attention_focused else 0.0,
                    'head_pose': self.criteria_weights['head_pose'] if attention_focused else 0.0
                }
            
            total_score = sum(score_components.values())
            
            # ì¶œì„ ìƒíƒœ ê²°ì •
            if total_score >= 0.8:
                status = "excellent"
            elif total_score >= 0.7:
                status = "present"
            elif total_score >= 0.5:
                status = "attention_needed"
            else:
                status = "absent"
            
            with self.analysis_lock:
                return {
                    'face_detected': face_detected,
                    'face_recognized': face_detected,
                    'is_alive': is_alive,
                    'attention_focused': attention_focused,
                    'head_pose': self.head_pose_angles.copy(),
                    'blink_count': self.blink_count,
                    'attendance_score': total_score,
                    'attendance_status': status,
                    'score_components': score_components,
                    'timestamp': time.time()
                }
                
        except Exception as e:
            logger.error(f"í”„ë ˆì„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_default_result()
    
    def _get_default_result(self):
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜"""
        with self.analysis_lock:
            return {
                'face_detected': False,
                'face_recognized': False,
                'is_alive': False,
                'attention_focused': False,
                'head_pose': self.head_pose_angles.copy(),
                'blink_count': self.blink_count,
                'attendance_score': 0.0,
                'attendance_status': "error",
                'score_components': {},
                'timestamp': time.time()
            }

# WebRTC ë¹„ë””ì˜¤ í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤
class VideoProcessor:
    def __init__(self, criteria_weights=None):
        self.frame_count = 0
        self.analysis_interval = 5
        
        try:
            self.analyzer = AttendanceAnalyzer(criteria_weights)
            self.last_analysis = None
            self.analysis_lock = Lock()
            logger.info("VideoProcessor ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"VideoProcessor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.analyzer = None
    
    def update_criteria_weights(self, new_weights):
        """íŒë‹¨ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        if self.analyzer:
            self.analyzer.update_criteria_weights(new_weights)
    
    def recv(self, frame):
        try:
            if self.analyzer is None:
                logger.error("ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return frame
                
            img = frame.to_ndarray(format="bgr24")
            
            if self.frame_count % self.analysis_interval == 0:
                analysis_result = self.analyzer.analyze_frame(img)
                
                with self.analysis_lock:
                    self.last_analysis = analysis_result
            
            self.frame_count += 1
            self.draw_analysis_result(img)
            
            return av.VideoFrame.from_ndarray(img, format="bgr24")
            
        except Exception as e:
            logger.error(f"í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return frame
    
    def draw_analysis_result(self, frame):
        """ë¶„ì„ ê²°ê³¼ë¥¼ í”„ë ˆì„ì— í‘œì‹œ"""
        try:
            with self.analysis_lock:
                if self.last_analysis:
                    result = self.last_analysis
                    
                    status_colors = {
                        "excellent": (0, 255, 0),
                        "present": (0, 200, 255),
                        "attention_needed": (0, 165, 255),
                        "absent": (0, 0, 255),
                        "error": (255, 0, 255)
                    }
                    
                    status = result['attendance_status']
                    color = status_colors.get(status, (255, 255, 255))
                    
                    cv2.putText(frame, f"Status: {status.upper()}", (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                    cv2.putText(frame, f"Score: {result['attendance_score']:.2f}", (10, 60),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    cv2.putText(frame, f"Face: {'âœ“' if result['face_detected'] else 'âœ—'}", (10, 90),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0) if result['face_detected'] else (0, 0, 255), 2)
                    cv2.putText(frame, f"Alive: {'âœ“' if result['is_alive'] else 'âœ—'}", (10, 120),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0) if result['is_alive'] else (0, 0, 255), 2)
                    cv2.putText(frame, f"Attention: {'âœ“' if result['attention_focused'] else 'âœ—'}", (10, 150),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0) if result['attention_focused'] else (0, 0, 255), 2)
                    
        except Exception as e:
            logger.error(f"ê²°ê³¼ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}")
            cv2.putText(frame, f"Error: {str(e)[:30]}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
    
    def get_last_analysis(self):
        """ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        with self.analysis_lock:
            if self.last_analysis:
                return self.last_analysis.copy()
            else:
                return {
                    'face_detected': False,
                    'face_recognized': False,
                    'is_alive': False,
                    'attention_focused': False,
                    'head_pose': {'yaw': 0, 'pitch': 0, 'roll': 0},
                    'blink_count': 0,
                    'attendance_score': 0.0,
                    'attendance_status': "error",
                    'score_components': {},
                    'timestamp': time.time()
                }

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'socketio_client' not in st.session_state:
        st.session_state.socketio_client = SocketIOClient()
    
    if 'video_processor' not in st.session_state:
        st.session_state.video_processor = None
        
        
    st.title("ğŸ“ ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ ê°•ì˜ ì¶œì„ ì‹œìŠ¤í…œ")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ì„¤ì •")
    
    # í•™ìƒ ì •ë³´ ì…ë ¥
    student_id = st.sidebar.text_input("í•™ìƒ ID", value="2024001")
    student_name = st.sidebar.text_input("í•™ìƒ ì´ë¦„", value="ê¹€ì² ìˆ˜")
    
    # ì„œë²„ ì—°ê²° ìƒíƒœ
    st.sidebar.subheader("ğŸŒ ì„œë²„ ì—°ê²°")
    connection_status = st.sidebar.empty()
    
    if st.sidebar.button("ì„œë²„ ì—°ê²°"):
        if st.session_state.socketio_client.connect_to_server():
            st.sidebar.success("ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.sidebar.error("ì„œë²„ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # ì—°ê²° ìƒíƒœ í‘œì‹œ
    if st.session_state.socketio_client.connected:
        connection_status.success("âœ… ì„œë²„ ì—°ê²°ë¨")
    else:
        connection_status.error("âŒ ì„œë²„ ë¯¸ì—°ê²°")
    
    # íŒë‹¨ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ì„¤ì •
    st.sidebar.subheader("ğŸ“Š íŒë‹¨ ê¸°ì¤€ ì„¤ì •")
    st.sidebar.write("ê° í•­ëª©ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•˜ì„¸ìš” (í•©ê³„: 1.0)")
    
    face_weight = st.sidebar.slider("ì–¼êµ´ ê²€ì¶œ", 0.0, 1.0, 0.2, 0.05)
    recognition_weight = st.sidebar.slider("ì–¼êµ´ ì¸ì‹", 0.0, 1.0, 0.3, 0.05)
    liveness_weight = st.sidebar.slider("ìƒì²´ í™œë™ì„±", 0.0, 1.0, 0.15, 0.05)
    attention_weight = st.sidebar.slider("ì£¼ì˜ì§‘ì¤‘ë„", 0.0, 1.0, 0.25, 0.05)
    pose_weight = st.sidebar.slider("ë¨¸ë¦¬ ìì„¸", 0.0, 1.0, 0.1, 0.05)
    
    # ê°€ì¤‘ì¹˜ í•©ê³„ í™•ì¸
    total_weight = face_weight + recognition_weight + liveness_weight + attention_weight + pose_weight
    st.sidebar.write(f"**ì´ ê°€ì¤‘ì¹˜: {total_weight:.2f}**")
    
    if abs(total_weight - 1.0) > 0.01:
        st.sidebar.warning("âš ï¸ ê°€ì¤‘ì¹˜ í•©ê³„ê°€ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤!")
    else:
        st.sidebar.success("âœ… ê°€ì¤‘ì¹˜ ì„¤ì • ì™„ë£Œ")
    
    # ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    criteria_weights = {
        'face_detected': face_weight,
        'face_recognized': recognition_weight,
        'liveness': liveness_weight,
        'attention': attention_weight,
        'head_pose': pose_weight
    }
    
    # WebRTC ì„¤ì •
    RTC_CONFIGURATION = RTCConfiguration(
        {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )
    
    # ë©”ì¸ ì»¨í…ì¸  ì˜ì—­
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“¹ ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ë¶„ì„")
        
        # VideoProcessor ì´ˆê¸°í™”/ì—…ë°ì´íŠ¸
        if st.session_state.video_processor is None:
            st.session_state.video_processor = VideoProcessor(criteria_weights)
        else:
            st.session_state.video_processor.update_criteria_weights(criteria_weights)
        
        # WebRTC ìŠ¤íŠ¸ë¦¬ë¨¸
        webrtc_ctx = webrtc_streamer(
            key="attendance-analyzer",
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=RTC_CONFIGURATION,
            video_processor_factory=lambda: st.session_state.video_processor,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=False,
        )
    
    with col2:
        st.subheader("ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼")
        status_placeholder = st.empty()
        metrics_placeholder = st.empty()
        weights_placeholder = st.empty()
        
        # ê°€ì¤‘ì¹˜ í‘œì‹œ
        with weights_placeholder.container():
            st.write("**í˜„ì¬ ê°€ì¤‘ì¹˜ ì„¤ì •:**")
            for key, value in criteria_weights.items():
                st.write(f"- {key}: {value:.2f}")
        
        # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë£¨í”„
        if webrtc_ctx.state.playing and webrtc_ctx.video_processor:
            while True:
                try:
                    result = webrtc_ctx.video_processor.get_last_analysis()
                    
                    if result and result['timestamp'] > 0:
                        # ì„œë²„ë¡œ ë°ì´í„° ì „ì†¡
                        if st.session_state.socketio_client.connected:
                            student_data = {
                                'id': student_id,
                                'name': student_name
                            }
                            st.session_state.socketio_client.send_attendance_data(
                                student_data, result
                            )
                        
                        # UI ì—…ë°ì´íŠ¸
                        with status_placeholder.container():
                            status_color = {
                                "excellent": "ğŸŸ¢",
                                "present": "ğŸŸ¡",
                                "attention_needed": "ğŸŸ ",
                                "absent": "ğŸ”´",
                                "error": "âš«"
                            }
                            
                            st.metric(
                                "ì¶œì„ ìƒíƒœ",
                                f"{status_color.get(result.get('attendance_status', 'error'), 'âšª')} {result.get('attendance_status', 'ERROR').upper()}",
                                f"ì ìˆ˜: {result.get('attendance_score', 0.0):.2f}"
                            )
                        
                        with metrics_placeholder.container():
                            col_a, col_b = st.columns(2)
                            with col_a:
                                st.metric("ì–¼êµ´ ê²€ì¶œ", "âœ…" if result.get('face_detected') else "âŒ")
                                st.metric("ìƒì²´ í™œë™ì„±", "âœ…" if result.get('is_alive') else "âŒ")
                            with col_b:
                                st.metric("ì–¼êµ´ ì¸ì‹", "âœ…" if result.get('face_recognized') else "âŒ")
                                st.metric("ì£¼ì˜ì§‘ì¤‘", "âœ…" if result.get('attention_focused') else "âŒ")
                            
                            st.subheader("ë¨¸ë¦¬ ìì„¸")
                            pose = result.get('head_pose', {'yaw':0, 'pitch':0, 'roll':0})
                            st.write(f"**Yaw:** {pose.get('yaw',0):.1f}Â°")
                            st.write(f"**Pitch:** {pose.get('pitch',0):.1f}Â°")
                            st.write(f"**Roll:** {pose.get('roll',0):.1f}Â°")
                            
                            st.metric("ëˆˆ ê¹œë°•ì„", result.get('blink_count', 0))
                            
                            # ì ìˆ˜ êµ¬ì„± ìš”ì†Œ í‘œì‹œ
                            if 'score_components' in result:
                                st.subheader("ì ìˆ˜ êµ¬ì„±")
                                for component, score in result['score_components'].items():
                                    st.write(f"- {component}: {score:.3f}")
                    else:
                        with status_placeholder.container():
                            st.info("ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
                    
                    time.sleep(0.2)
                except Exception as e:
                    logger.error(f"ê²°ê³¼ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}")
                    with status_placeholder.container():
                        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    break
                
                if not webrtc_ctx.state.playing:
                    break
        else:
            with status_placeholder.container():
                st.info("ì›¹ìº ì„ ì‹œì‘í•˜ë ¤ë©´ 'START' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
    
    # í•˜ë‹¨ ì •ë³´ í‘œì‹œ
    st.markdown("---")
    with st.expander("ğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´"):
        st.write("**ë¶„ì„ í•­ëª©:**")
        st.write("- ì–¼êµ´ ê²€ì¶œ: ì¹´ë©”ë¼ì—ì„œ ì–¼êµ´ì´ ì •ìƒì ìœ¼ë¡œ ê²€ì¶œë˜ëŠ”ì§€ í™•ì¸")
        st.write("- ì–¼êµ´ ì¸ì‹: ë“±ë¡ëœ í•™ìƒì˜ ì–¼êµ´ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸")
        st.write("- ìƒì²´ í™œë™ì„±: ëˆˆ ê¹œë°•ì„ ë“±ì„ í†µí•œ ì‹¤ì œ ì‚¬ëŒ ì—¬ë¶€ í™•ì¸")
        st.write("- ì£¼ì˜ì§‘ì¤‘ë„: í™”ë©´ì„ ë°”ë¼ë³´ê³  ìˆëŠ”ì§€ ì§‘ì¤‘ë„ ì¸¡ì •")
        st.write("- ë¨¸ë¦¬ ìì„¸: ì˜¬ë°”ë¥¸ ìˆ˜ê°• ìì„¸ ìœ ì§€ ì—¬ë¶€")
        
        st.write("**ì¶œì„ ë“±ê¸‰:**")
        st.write("- ğŸŸ¢ ìš°ìˆ˜ (0.8 ì´ìƒ): ë§¤ìš° ì§‘ì¤‘ì  ìˆ˜ê°•")
        st.write("- ğŸŸ¡ ì •ìƒ (0.7 ì´ìƒ): ì–‘í˜¸í•œ ì°¸ì—¬ë„")
        st.write("- ğŸŸ  ì£¼ì˜ (0.5 ì´ìƒ): ì‚°ë§Œí•˜ê±°ë‚˜ ë¶€ë¶„ ì°¸ì—¬")
        st.write("- ğŸ”´ ë¹„ì •ìƒ (0.5 ë¯¸ë§Œ): ì¶œì„ ë¶ˆì¸ì •")
        
        st.write("**ì„œë²„ í†µì‹ :**")
        st.write("- SocketIOë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì„œë²„ í†µì‹ ")
        st.write("- ë¶„ì„ ê²°ê³¼ ìë™ ì „ì†¡ ë° ê¸°ë¡")
        st.write("- ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§")
        
        st.write("**êµìˆ˜ì ì„¤ì •:**")
        st.write("- íŒë‹¨ ê¸°ì¤€ë³„ ê°€ì¤‘ì¹˜ ì‹¤ì‹œê°„ ì¡°ì • ê°€ëŠ¥")
        st.write("- ê°•ì˜ íŠ¹ì„±ì— ë§ëŠ” ë§ì¶¤í˜• ì¶œì„ ê¸°ì¤€ ì„¤ì •")
        st.write("- ì„¤ì • ë³€ê²½ ì‹œ ì¦‰ì‹œ ë°˜ì˜")

if __name__ == "__main__":
    main()
